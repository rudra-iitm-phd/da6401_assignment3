# ğŸŒğŸ”¤ Transliteration with Seq2Seq + Attention âš¡

> ğŸš€ A robust and configurable character-level transliteration model built using PyTorch, featuring RNN/LSTM/GRU backbones, optional attention, beam search, and connectivity visualization. Developed as part of **DA6401: Advanced Machine Learning** at IIT Madras.

---

ğŸ“˜ **[ğŸ“Š Click here to view the detailed W&B Report â†’](https://wandb.ai/da24d008-iit-madras/da6401-assignment3/reports/DA6401-Assignment-3--VmlldzoxMjg0NjYyNA?accessToken=90e7xuf7uiyi5d5v1hbbsjopkb7p5j5aw54ez8l64b9khg9qkupcms2nzpoyj42j)**

---
## ğŸ§  Key Features

- ğŸ” **Encoder-Decoder Architecture** (RNN / LSTM / GRU)
- ğŸ§² **Attention Mechanism** (Bahdanau-style additive attention)
- ğŸ§ª **Supports Beam Search** decoding
- ğŸ“Š **Hyperparameter Sweeps** with Weights & Biases
- ğŸ“ˆ **Interactive Attention Visualization** with HTML & JS
- ğŸŒ **Supports Multiple Indian Languages** (Dakshina dataset)
- ğŸ¯ **Modular Design** â€“ easy to plug and play components

## ğŸ—‚ï¸ Project Structure

```bash
ğŸ“¦ da6401_assignment3/
â”‚
â”œâ”€â”€ ğŸ§  Core Model
â”‚   â”œâ”€â”€ model.py              # Dynamic Seq2Seq model with optional attention
â”‚   â””â”€â”€ shared.py             # Global vocab mappings (char2idx, idx2char)
â”‚
â”œâ”€â”€ ğŸ§¾ Data & Preprocessing
â”‚   â”œâ”€â”€ data.py               # Dataset class for loading & encoding Dakshina data
â”‚   â””â”€â”€ configure.py          # Loads dataset, builds model and optimizer
â”‚
â”œâ”€â”€ ğŸ‹ï¸â€â™‚ï¸ Training & Evaluation
â”‚   â”œâ”€â”€ train.py              # Training loop, evaluation, logging, visualization
â”‚   â”œâ”€â”€ best_configs.py       # Predefined high-performing hyperparameter sets
â”‚   â”œâ”€â”€ sweep_configuration.py# W&B hyperparameter sweep configurations
â”‚   â””â”€â”€ argument_parser.py    # Command-line arguments for configuration
â”‚
â”œâ”€â”€ ğŸŒ Visualization
â”‚   â””â”€â”€ connectivity_html.py  # HTML-based attention visualization with Devanagari support
â”‚
â”œâ”€â”€ ğŸ“‚ Output & Logs
â”‚   â”œâ”€â”€ predictions_vanilla/  # Stores predictions from the vanilla model
â”‚   â”œâ”€â”€ wandb/                # W&B run logs and metadata
â”‚   â””â”€â”€ __pycache__/          # Python bytecode cache (auto-generated)
|   â””â”€â”€predictions_attention/  # Stores predictions from the attention model
â”‚
â”œâ”€â”€ ğŸ“„ README.md              # Project overview and documentation
â””â”€â”€ ğŸ“„ .DS_Store              # (Optional) macOS system metadata file
```

## ğŸƒâ€â™‚ï¸ Quickstart

### 1. ğŸ§¹ Install Dependencies

```bash
python -m pip install python>=3.7 torch>=1.7 numpy pandas matplotlib seaborn tqdm wandb requests
```

### 2. ğŸ“¦ Train a Model

```bash
python train.py -n hi -m lstm -ua True --wandb
```

> Use `--help` to see all CLI options.

## ğŸ–¼ï¸ Attention Connectivity Visualization

> ğŸ§  Gain insights into model behavior with interactive HTML-based attention maps.

âœ… Shows attention links between characters  
âœ… Highlights top-3 predicted tokens with probabilities  
âœ… Supports Devanagari font rendering  

## ğŸ§  Model Architecture

```
Input (Latin)
   â†“
Embedding â†’ Encoder (RNN/LSTM/GRU) â”€â”€â–º Hidden States
                                          â†“
                              Attention Context (optional)
                                          â†“
Decoder (with or without attention) â†’ Output (Native Script)
```

## ğŸ“Š Example Hyperparameters (Best Configs)

### ğŸ”¥ Attention Model
```python
{
  "activation": "relu",
  "batch_size": 128,
  "dropout_rate": 0.6,
  "epochs": 20,
  "model": "lstm",
  ...
}
```

## ğŸ“š Dataset

ğŸ—‚ï¸ [Dakshina Dataset v1.0](https://github.com/google-research-datasets/dakshina)  
ğŸ“ Expected path: `../dakshina_dataset_v1.0/<lang>/lexicons/<lang>.translit.sampled.{train,dev,test}.tsv`

## ğŸ“‹ Examples

Run with the default vanilla seq2seq settings and log it to wandb:
```bash
python train.py --wandb
```

Run with the attention integrated with seq2seq settings and log it to wandb:
```bash
python train.py --wandb -ua True
```

Run and evaluate on test set and log it to wandb with the best configuration:
```bash
python train.py --wandb --use_test
```

Run and evaluate on test set with attention and log it to wandb along with logging heatmaps and the connectivity diagram:
```bash
python train.py --use_test --wandb -ua True -lc True 
```

Initialize a wandb sweep for the vanilla setting:
```bash
python train.py --wandb_sweep
```

Initialize a wandb sweep for the attention setting:
```bash
python train.py --wandb_sweep -ua True
```

## ğŸ”§ CLI Arguments & Configuration Options

Here are all the command-line options you can use with `train.py`, based on `argument_parser.py`:

| ğŸ·ï¸ **Flag** | ğŸ”‘ **Argument** | ğŸ’¬ **Description** | ğŸ§  **Type** | ğŸ¯ **Default** |
|-------------|------------------|--------------------|------------|----------------|
| `-b`        | `--batch_size`      | ğŸ“¦ Batch size | `int`     | `128`          |
| `-n`        | `--native`          | ğŸŒ Native language code (e.g. `hi`, `bn`, `ta`, etc.) | `str` | `'hi'` |
| `-m`        | `--model`           | ğŸ§  Model type: `rnn`, `lstm`, `gru` | `str` | `'lstm'` |
| `-eed`      | `--enc_embedding_dim` | ğŸ§± Encoder embedding size | `int` | `64` |
| `-ed`       | `--enc_dim`         | ğŸ”¢ Encoder hidden size | `int` | `512` |
| `-ne`       | `--n_encoders`      | ğŸ“š Number of encoder layers | `int` | `1` |
| `-ded`      | `--dec_embedding_dim` | ğŸ§± Decoder embedding size | `int` | `64` |
| `-dd`       | `--dec_dim`         | ğŸ”¢ Decoder hidden size | `int` | `512` |
| `-nd`       | `--n_decoders`      | ğŸ“š Number of decoder layers | `int` | `1` |
| `-ld`       | `--linear_dim`      | ğŸ“ Hidden size of the linear layer | `int` | `2048` |
| `-do`       | `--dropout_rate`    | ğŸ’§ Dropout probability | `float` | `0.4` |
| `-e`        | `--epochs`          | â³ Number of training epochs | `int` | `20` |
| `-ua`       | `--use_attn`        | ğŸ§² Enable attention mechanism | `bool` | `False` |
| `-lc`       | `--log_connectivity`| ğŸ–¼ï¸ Log HTML attention visualization | `bool` | `False` |
| `-mom`      | `--momentum`        | ğŸŒ€ Optimizer momentum (for Adam family) | `float` | `0.935939775` |
| `-k`        | `--beam_size`       | ğŸŒ¬ï¸ Beam width for beam search | `int` | `3` |
| `-a`        | `--activation`      | âš™ï¸ Activation function: `relu`, `tanh`, `gelu` | `str` | `'relu'` |
| `-o`        | `--optimizer`       | ğŸ§ª Optimizer: `adam`, `adamw`, `adamax`, `rmsprop` | `str` | `'adamw'` |
| `-lr`       | `--learning_rate`   | ğŸ§® Learning rate | `float` | `0.00319` |
| `--use_v2`  |                    | ğŸ§ª Use alternate attention configuration (v2) | `bool` | `False` |
| `--use_test`|                    | ğŸ§ª Evaluate on test set after training | `bool` | `False` |
| `--wandb`   |                    | ğŸ“Š Enable Weights & Biases logging | `bool` | `False` |
| `-we`       | `--wandb_entity`    | ğŸ§‘â€ğŸ”¬ W&B entity name | `str` | `'da24d008-iit-madras'` |
| `-wp`       | `--wandb_project`   | ğŸ“ W&B project name | `str` | `'da6401-assignment3'` |
| `--wandb_sweep` |               | ğŸ”„ Enable W&B sweep mode | `bool` | `False` |
| `--sweep_id`|                    | ğŸ†” W&B sweep ID to resume | `str` | `None` |

## ğŸ§‘â€ğŸ’» Author

ğŸ‘¤ Developed by Rudra Sarkar, DA24D008, PhD, Data Science and Artificial Intelligence Dept, IIT Madras  
ğŸ“¬ Feel free to reach out for academic collaborations or extensions!

## ğŸ›¡ï¸ License

This project is licensed for academic and educational use.  
If adapting, please **cite or attribute** appropriately.

## â­ Acknowledgements

- ğŸ¤— [PyTorch](https://pytorch.org/)
- ğŸ§ª [Weights & Biases](https://wandb.ai/)
- ğŸ‡®ğŸ‡³ [Dakshina Dataset](https://github.com/google-research-datasets/dakshina)

---

### ğŸŒŸ If you find this useful, consider leaving a â­ on GitHub!
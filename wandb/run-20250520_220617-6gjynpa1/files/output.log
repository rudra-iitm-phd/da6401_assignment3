/opt/anaconda3/envs/nlp_env/lib/python3.12/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
  2%|███▎                                                                                                                                         | 8/346 [00:02<01:38,  3.45it/s]
Traceback (most recent call last):
  File "/Users/rudra_sarkar/Documents/PhD IIT Madras/Intro to Deep Learning M Khapra DA6401/da6401_assignment_3/train.py", line 324, in <module>
    train_core(log=True, sweep=False)
  File "/Users/rudra_sarkar/Documents/PhD IIT Madras/Intro to Deep Learning M Khapra DA6401/da6401_assignment_3/train.py", line 282, in train_core
    trained_model = train_model(model, config['epochs'], train_dl, val_dl, criterion, optimizer, device, beam_size=config['beam_size'], log=log)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/rudra_sarkar/Documents/PhD IIT Madras/Intro to Deep Learning M Khapra DA6401/da6401_assignment_3/train.py", line 132, in train_model
    optimizer.step()
  File "/opt/anaconda3/envs/nlp_env/lib/python3.12/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/nlp_env/lib/python3.12/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/nlp_env/lib/python3.12/site-packages/torch/optim/adamw.py", line 187, in step
    adamw(
  File "/opt/anaconda3/envs/nlp_env/lib/python3.12/site-packages/torch/optim/adamw.py", line 339, in adamw
    func(
  File "/opt/anaconda3/envs/nlp_env/lib/python3.12/site-packages/torch/optim/adamw.py", line 419, in _single_tensor_adamw
    exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)
    ^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
